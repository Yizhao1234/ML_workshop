{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Regression\n",
    "Linear models are a class of models that are widely used in practice and have been studied extensively in the last few decades, with roots going back over a hundred years. Here, we will use linear models for developing regressors. Let's load the packages and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "import urllib.request\n",
    "\n",
    "print('Beginning file download with url...')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mohammadrashedi/ML_workshop/master/CommViolPredUnnormalizedData.txt'\n",
    "urllib.request.urlretrieve(url, 'CommViolPredUnnormalizedData.txt')\n",
    "\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "def load_crime_dataset():\n",
    "    # Communities and Crime dataset for regression\n",
    "    # https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n",
    "\n",
    "    crime = pd.read_table('CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "    # remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n",
    "    columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n",
    "    crime = crime.iloc[:,columns_to_keep].dropna()\n",
    "\n",
    "    X_crime = crime.iloc[:,range(0,88)]\n",
    "    y_crime = crime['ViolentCrimesPerPop']\n",
    "\n",
    "    return (X_crime, y_crime)\n",
    "(X_crime, y_crime) = load_crime_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the above code, we have generated a set of data as `X_R1` and `y_R1` and imported another dataset as `X_crime` and `y_crime`. Next, we visualize the regression data that we have generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also display the crime data in a table as follows. This dataset seems to be for the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_table('CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]\n",
    "crime = crime.iloc[:,columns_to_keep].dropna()\n",
    "crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "First, we train a linear model for the generated regression data (X_R1 and y_R1). Please note that there is only one column of data for X_R1, i.e. the output is modeled based on one feature only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,random_state=0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the trained model within the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we train the model based on the crime data, which uses 89 features for predicting the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "As an exercise, fit a linear model to the crime dataset by using `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime, random_state = 0)\n",
    "linreg = # Enter your code here.\n",
    "\n",
    "print('Crime dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Since there are 89 features with the crime data, we might get a better result by using regularization. So, first we try Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get a better result by feature scaling (normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we explore the effect of regularization parameter, `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnumber of abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "In another attempt, let's try Lasso regression which uses L-1 norm in the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the regularization parameter and see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Here we will see the use case of logistic regression for binary classification. The dataset includes the properties of some fruits. Let's load the data and see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mohammadrashedi/ML_workshop/master/adspy_shared_utilities.py'\n",
    "urllib.request.urlretrieve(url, 'adspy_shared_utilities.py')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (plot_class_regions_for_classifier_subplot)\n",
    "print('Beginning file download with url...')\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mohammadrashedi/ML_workshop/master/fruit_data_with_colors.txt'\n",
    "urllib.request.urlretrieve(url, 'fruit_data_with_colors.txt')\n",
    "\n",
    "fruits = pd.read_table('fruit_data_with_colors.txt')\n",
    "fruits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we would like to classify the fruits as apples and non-apples. So, those fruits with label=1 will be apple with and their label will be changed to 1 while the label of others will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n",
    "y_fruits_apple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model and plot the result of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.as_matrix(),\n",
    "                y_fruits_apple.as_matrix(),\n",
    "                random_state = 0))\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n",
    "                                         None, 'Logistic regression \\\n",
    "for binary classification\\nFruit dataset: Apple vs others',\n",
    "                                         subaxes)\n",
    "\n",
    "h = 6\n",
    "w = 8\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "\n",
    "h = 10\n",
    "w = 7\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "subaxes.set_xlabel('height')\n",
    "subaxes.set_ylabel('width')\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
